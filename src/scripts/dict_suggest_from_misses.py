from __future__ import annotations

import argparse
import asyncio
import logging
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable

import asyncpg

from src.config import Settings
from src.analysis.classifier import _norm_text, _tokenize  # reuse same normalization


log = logging.getLogger(__name__)


_CODE_RX = re.compile(r"\b[a-z]{1,6}\d{2,5}[a-z]{0,4}\b", re.I)
# Примеры: t480, 5552g, e728, x558, 840g5, an515, g5, 15ach6, 14alc05 и т.п.


@dataclass(frozen=True)
class MissRow:
    id: int
    title: str
    description: str | None
    url: str | None


def _extract_codes(text: str) -> set[str]:
    """
    Достаёт "модельные коды" из нормализованного текста.
    Стараемся не быть слишком агрессивными: важнее precision, чем recall на этом этапе.
    """
    t = _norm_text(text)
    hits = {m.group(0).lower() for m in _CODE_RX.finditer(t)}
    # чуть нормализации для "g5 15" и подобных мы делаем через _tokenize ниже,
    # тут оставляем только слитные коды.
    return hits


def _guess_brand_token(tokens: set[str], known_brands: set[str]) -> str | None:
    """
    Грубая эвристика: если среди токенов есть бренд из brands.name_norm — используем его.
    """
    for b in known_brands:
        if b in tokens:
            return b
    return None


def _render_sql_patch(rows: list[dict], *, out_path: Path) -> None:
    """
    Пишем SQL так, чтобы:
    - добавить brand (если надо)
    - добавить family (placeholder каноническое имя)
    - добавить token-алиас для семейства
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)

    lines: list[str] = []
    lines.append("-- AUTOGENERATED: dictionary patch from misses")
    lines.append("-- Review before applying.")
    lines.append("BEGIN;")
    lines.append("")

    # Бренды: добавляем только те, которых нет.
    brands = sorted({r["brand_norm"] for r in rows if r["brand_norm"]})
    if brands:
        lines.append("-- 1) Brands")
        lines.append("INSERT INTO brands(name, name_norm)")
        lines.append("VALUES")
        for i, b in enumerate(brands):
            # canonical name = Capitalized brand_norm (достаточно как placeholder)
            name = b.capitalize()
            comma = "," if i < len(brands) - 1 else ""
            lines.append(f"  ('{name}', '{b}'){comma}")
        lines.append("ON CONFLICT (name_norm) DO NOTHING;")
        lines.append("")

    # Семейства: привязываем к бренду.
    lines.append("-- 2) Model families (placeholder canonical names)")
    lines.append("-- family_name можно потом переименовать вручную, это не ломает алиасы.")
    for r in rows:
        b = r["brand_norm"]
        fam_norm = r["family_norm"]
        fam_name = r["family_name"]
        lines.append(
            "INSERT INTO model_families(category, brand_id, family_name, family_name_norm)\n"
            f"SELECT 'laptop', b.id, '{fam_name}', '{fam_norm}'\n"
            "FROM brands b\n"
            f"WHERE b.name_norm = '{b}'\n"
            "ON CONFLICT (brand_id, family_name_norm) DO NOTHING;"
        )
    lines.append("")

    # Алиасы токены (сам код/маркер)
    lines.append("-- 3) Family token aliases")
    for r in rows:
        b = r["brand_norm"]
        fam_norm = r["family_norm"]
        token = r["token"]
        weight = r["weight"]
        lines.append(
            "INSERT INTO model_aliases(family_id, match_type, pattern, weight)\n"
            "SELECT mf.id, 'token', x.pattern, x.weight\n"
            "FROM (\n"
            f"  VALUES ('{fam_norm}', '{token}', {weight})\n"
            ") AS x(family_name_norm, pattern, weight)\n"
            "JOIN brands b ON b.name_norm = x.family_name_norm::text IS NOT NULL\n"
            "JOIN model_families mf ON mf.family_name_norm = x.family_name_norm\n"
            "WHERE mf.brand_id = (SELECT id FROM brands WHERE name_norm = "
            f"'{b}' LIMIT 1)\n"
            "AND NOT EXISTS (\n"
            "  SELECT 1 FROM model_aliases ma\n"
            "  WHERE ma.family_id = mf.id AND ma.match_type='token' AND ma.pattern=x.pattern\n"
            ");"
        )

    lines.append("")
    lines.append("COMMIT;")
    out_path.write_text("\n".join(lines), encoding="utf-8")
    log.info("SQL patch written: %s", out_path)


async def _load_known_brands(pool: asyncpg.Pool) -> set[str]:
    sql = "SELECT name_norm FROM brands"
    async with pool.acquire() as conn:
        rows = await conn.fetch(sql)
    brands = {str(r["name_norm"]).strip() for r in rows if r["name_norm"]}
    return {b for b in brands if b}


async def _fetch_misses(pool: asyncpg.Pool, *, limit: int) -> list[MissRow]:
    sql = """
    SELECT id, title, description, url
    FROM items
    WHERE category = 'laptop'
      AND (model_family_id IS NULL AND model_variant_id IS NULL)
    ORDER BY last_seen_at DESC
    LIMIT $1
    """
    async with pool.acquire() as conn:
        rows = await conn.fetch(sql, limit)
    out: list[MissRow] = []
    for r in rows:
        out.append(
            MissRow(
                id=int(r["id"]),
                title=str(r["title"] or ""),
                description=str(r["description"]) if r["description"] is not None else None,
                url=str(r["url"]) if r["url"] is not None else None,
            )
        )
    return out


async def run(*, limit: int, top: int, out_sql: Path, min_freq: int) -> None:
    s = Settings()
    logging.basicConfig(level=getattr(logging, s.log_level.upper(), logging.INFO))

    log.info("starting dict_suggest_from_misses: limit=%s top=%s min_freq=%s", limit, top, min_freq)

    pool = await asyncpg.create_pool(dsn=s.pg_dsn, min_size=1, max_size=5)
    try:
        known_brands = await _load_known_brands(pool)
        log.info("known brands loaded: %s", len(known_brands))

        misses = await _fetch_misses(pool, limit=limit)
        log.info("misses fetched: %s", len(misses))

        # Агрегация (brand_norm, code) -> freq
        freq: dict[tuple[str, str], int] = {}

        for m in misses:
            text = f"{m.title or ''} {m.description or ''}"
            norm = _norm_text(text)
            tokens = _tokenize(norm)

            brand = _guess_brand_token(tokens, known_brands)
            if not brand:
                continue

            # 1) слитные коды
            codes = _extract_codes(norm)

            # 2) добавим “склеенные” токены типа 840g5, nitro5, air13 (они могут появляться уже как token)
            #    Берём только токены, где есть цифры.
            codes |= {t for t in tokens if any(ch.isdigit() for ch in t) and 3 <= len(t) <= 12}

            # фильтр мусора: убираем частые не-модели
            junk = {"2020", "2021", "2022", "2023", "2024", "2025", "2026", "win10", "win11", "rtx", "gtx"}
            codes = {c for c in codes if c not in junk}

            for c in codes:
                freq[(brand, c)] = freq.get((brand, c), 0) + 1

        # Берём top по частоте, но только >= min_freq
        ranked = sorted(freq.items(), key=lambda kv: kv[1], reverse=True)
        ranked = [(k, n) for k, n in ranked if n >= min_freq][:top]

        log.info("candidates selected: %s", len(ranked))
        if not ranked:
            log.warning("no candidates found. try increase limit or lower min_freq.")
            return

        rows_for_sql: list[dict] = []
        for (brand_norm, code), n in ranked:
            # Placeholder family naming: "Brand <CODE>"
            family_name = f"{brand_norm.capitalize()} {code.upper()}"
            family_norm = _norm_text(f"{brand_norm} {code}").strip()

            rows_for_sql.append(
                {
                    "brand_norm": brand_norm,
                    "token": code.lower(),
                    "weight": min(10, 3 + (n // 3)),  # растёт с частотой, но ограничен
                    "family_name": family_name,
                    "family_norm": family_norm,
                    "freq": n,
                }
            )

        _render_sql_patch(rows_for_sql, out_path=out_sql)

        # Дополнительно печатаем топ в лог
        for x in rows_for_sql[:25]:
            log.info(
                "candidate: brand=%s token=%s freq=%s family_norm=%s",
                x["brand_norm"],
                x["token"],
                x["freq"],
                x["family_norm"],
            )

    finally:
        await pool.close()


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Generate dictionary SQL patch from unclassified items.")
    p.add_argument("--limit", type=int, default=5000, help="How many recent misses to analyze.")
    p.add_argument("--top", type=int, default=200, help="How many candidates to output.")
    p.add_argument("--min-freq", type=int, default=6, help="Minimum occurrences to include.")
    p.add_argument("--out-sql", type=str, default="var/dictionary_patch.sql", help="Output SQL file path.")
    return p.parse_args()


if __name__ == "__main__":
    a = parse_args()
    asyncio.run(
        run(
            limit=int(a.limit),
            top=int(a.top),
            min_freq=int(a.min_freq),
            out_sql=Path(a.out_sql),
        )
    )